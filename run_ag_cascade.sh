# Jul 20
# validation
# python runbenchmark.py autogluon_v0.5.1 validation -u user_dir
# python runbenchmark.py autogluon_v0.5.1 validation

# POC with infer_limit
# python runbenchmark.py autogluon_v0.5.1_high_il0.001 example 1h8c -u user_dir -m aws
# need to get rid of -u arg, so now directly change resources/config.yaml, resources/frameworks.yaml
# python runbenchmark.py autogluon_v0.5.1_high_il0.001 example 1h8c -m aws


# Jul 22, run one chunk of AutoML Benchmark All Classification (openml/s/271)
# ~10 dataset * 10 fold * 1h = 100 hour, using 10 instance costs 10 hours
# autogluon_v0.5.1_high_il0.001 is defined in `resources/frameworks.yaml`
# openml_bench_271-multiclass-chunk_0 is in `resources/benchmarks/openml_bench_271-multiclass-chunk_0.yaml`
#     generated by `scripts/process_openml_benchmark_suite.py`
# is it ok to use python 3.9?
# python runbenchmark.py autogluon_v0.5.1_high_il0.001 openml_bench_271-multiclass-chunk_0 1h8c -m aws -p 10

# Jul 24
# POC with infer_util for genuine infer speed based on infer_limit_batch_size
# python runbenchmark.py autogluon_v0.5.1_high_il0.001 example 1h8c -m local -f 0
# python runbenchmark.py autogluon_v0.5.1_high_il0.001 example 1h8c -m aws -f 0
# 30 dataset * 10 fold * 1h = 300 hour, using 300 instance can finish in 1 hour!
# python runbenchmark.py autogluon_v0.5.1_high_il0.001 openml_bench_271-multiclass 1h8c -m aws -p 300